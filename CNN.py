import sys
from PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QLabel, QVBoxLayout, QHBoxLayout, QFileDialog , QMessageBox
from PIL import Image
import torchvision.transforms as transforms
import torch
import torchvision
import torchvision.models as models
import torchvision.datasets as datasets
import torch.nn as nn
from PyQt5.QtCore import *
from PyQt5.QtGui import *



# d·ªØ li·ªáu
data_dir = 'dataset'
train_dir = data_dir + '/train'
val_dir = data_dir + '/val'
test_dir = data_dir + '/test'
# ƒë·ªãnh h√¨nh 
transform = transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
                                ])
# T·∫°o dataset
train_data = datasets.ImageFolder(train_dir, transform=transform)
val_data = datasets.ImageFolder(val_dir, transform=transform)
test_data = datasets.ImageFolder(test_dir, transform=transform)
# T·∫°o dataloader
train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)


# X√¢y d·ª±ng m√¥ h√¨nh
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.fc1 = nn.Linear(64 * 56 * 56, 512)
        self.fc2 = nn.Linear(512, len(train_data.classes))
    
    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.pool(nn.functional.relu(self.conv2(x)))
        x = torch.flatten(x, 1)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x
model = CNN()



### t·∫£i l·∫°i m√¥ h√¨nh ƒë√£ l∆∞u
# T·∫°o m√¥ h√¨nh gi·ªëng v·ªõi m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán tr∆∞·ªõc ƒë√≥
model = torchvision.models.resnet18()
# T·∫£i tr·ªçng s·ªë m√¥ h√¨nh ƒë√£ l∆∞u v√†o m√¥ h√¨nh m·ªõi
PATH = '../DoAnML/CNN/model.pth'
model.load_state_dict(torch.load(PATH))


# S·ª≠ d·ª•ng m√¥ h√¨nh ƒë·ªÉ d·ª± ƒëo√°n
input_data = torch.randn(1, 3, 224, 224)  # v√≠ d·ª• input data ng·∫´u nhi√™n
output = model(input_data)


# t·∫°o giao di·ªán
app = QApplication(sys.argv)
window = QWidget()
window.setWindowTitle('D·ª± ƒëo√°n tr√°i c√¢y v√† rau c·ªß qu·∫£  üçÖ')
window.setGeometry(100, 100, 400, 200)


# T·∫°o c√°c th√†nh ph·∫ßn giao di·ªán
image_path = None
image_label = QLabel('KH√îNG C√ì B·ª®C ·∫¢NH N√ÄO ƒê∆Ø·ª¢C CH·ªåN. üçÖ ')
image_label.setStyleSheet("background-color: #F0F0F0;font-weight:bold; font-size: 24px; color: red;")
image_label.setFont(QFont('Arial', 12))

image_display = QLabel()
image_display.setFixedSize(224, 224)

browse_button = QPushButton('T·∫£i ·∫£nh l√™n...')
browse_button.setStyleSheet("background-color: 'black'; font-size: 12pt; font-weight: bold; color:white")

predict_button = QPushButton('D·ª± ƒêo√°n')
predict_button.setStyleSheet("background-color: 'green' ; font-size: 12pt; font-weight: bold; color:white")


recovery_button = QPushButton('Kh√¥i Ph·ª•c ·∫¢nh G·ªëc')
recovery_button.setStyleSheet("background-color: 'Blue'; font-size: 12pt; font-weight: bold; color:white")

result_label = QLabel('')
result_label.setFont(QFont('Arial', 12))

# X·ª≠ l√Ω s·ª± ki·ªán khi n√∫t "Browse" ƒë∆∞·ª£c nh·∫•n
def browse_image():
    global image_path
    options = QFileDialog.Options()
    options |= QFileDialog.DontUseNativeDialog
    file_path, _ = QFileDialog.getOpenFileName(window, "Ch·ªçn ·∫£nh", "", "Image Files (*.jpg *.jpeg *.png *.bmp)", options=options)
    if file_path:
        image_path = file_path
        image_label.setText('·∫¢nh ƒë·∫ßu v√†o : ' + image_path)        
        # Chuy·ªÉn ƒë·ªïi h√¨nh ·∫£nh PIL th√†nh QImage
        img = Image.open(image_path)
        img = Image.open(image_path)
        img = img.resize((224, 224))
        gray_img = img.convert('L')  # Chuy·ªÉn ƒë·ªïi sang ·∫£nh m√†u x√°m
        qimage = QImage(gray_img.tobytes(), gray_img.size[0], gray_img.size[1], QImage.Format_Grayscale8)
        image_display.setPixmap(QPixmap.fromImage(qimage))


# X·ª≠ l√Ω s·ª± ki·ªán khi n√∫t "Predict" ƒë∆∞·ª£c nh·∫•n
def predict_image():
    
    if image_path:
        # ƒê·ªçc v√† ti·ªÅn x·ª≠ l√Ω h√¨nh ·∫£nh ƒë·∫ßu v√†o
        img = Image.open(image_path)
        transform = transforms.Compose([
            transforms.Resize((224,224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])
        img_tensor = transform(img)
        img_tensor = torch.unsqueeze(img_tensor, 0)        
     # D√πng m√¥ h√¨nh ƒë·ªÉ d·ª± ƒëo√°n
        model.eval()
        with torch.no_grad():
            output = model(img_tensor)
            _, predicted = torch.max(output.data, 1)
            
        # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ d·ª± ƒëo√°n sang lo·∫°i rau c·ªß hay tr√°i c√¢y v√† t√™n lo·∫°i
        class_names = train_data.classes
        if predicted.item() == 0:
            result_label.setText("·∫¢nh n√†y ƒë∆∞·ª£c ph√¢n lo·∫°i l√† m·ªôt lo·∫°i hoa qu·∫£  .\nT√™n lo·∫°i l√†: " + class_names[predicted.item()])
        else:
            result_label.setText("·∫¢nh n√†y ƒë∆∞·ª£c ph√¢n lo·∫°i l√† m·ªôt lo·∫°i rau c·ªß   .\nT√™n lo·∫°i l√†: " + class_names[predicted.item()])
            
            
# Recovery image
       
       
    else:
        message_box = QMessageBox()
        message_box.setWindowTitle("Th√¥ng b√°o")
        message_box.setText("H√£y ch·ªçn v√†o 1 b·ª©c ·∫£nh.")
        message_box.exec_()
        
        
def recovery_image():
     img_pixmap = QPixmap(image_path).scaled(image_display.size(), Qt.KeepAspectRatio)
     image_display.setPixmap(img_pixmap)



# K·∫øt n·ªëi c√°c th√†nh ph·∫ßn v·ªõi s·ª± ki·ªán
browse_button.clicked.connect(browse_image)
predict_button.clicked.connect(predict_image)
recovery_button.clicked.connect(recovery_image)

# B·ªë tr√≠ c√°c th√†nh ph·∫ßn tr√™n c·ª≠a s·ªï giao di·ªán
image_layout = QVBoxLayout()
image_layout.addWidget(image_label)
image_layout.addWidget(image_display)

button_layout = QHBoxLayout()
button_layout.addWidget(browse_button)

button_layout = QHBoxLayout()
button_layout.addWidget(browse_button)
button_layout.addWidget(predict_button)
button_layout.addWidget(recovery_button)


main_layout = QVBoxLayout()
main_layout.addLayout(image_layout)
main_layout.addLayout(button_layout)
main_layout.addWidget(result_label)





window.setLayout(main_layout)

window.show()
sys.exit(app.exec_())

